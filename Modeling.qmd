---
title: "Modeling"
format: html
editor: visual
---

## Modeling
This data have binary response variable diabetes_binary that gives whether there is diabetes or not. Our response variable is a categorical variable so a classification task, when we have categorical variable as response like success or failure we can use logistic regression model for predictions.

We will be using Classification Tree and Random Forest to create models for the Diabetes_binary variable. We will be using the following as predictors: BMI, Age, Sex, PhysActivity, HighChol, GenHlth variable.
After that we will select the best out the two models, using log-loss as metric. 

## Classification Tree
Classification tree allows us to classify group membership. It tries to split up the predictor space into regions forming a tree and use the most prevalent class in the region as prediction.

```{r}
library(tidyverse)
library(tidymodels)

set.seed(10)
```

Splitting data in training and testing set
```{r}
diabetes_bin_split <- initial_split(diabetes_bin, prop = 0.7)
diabetes_bin_train <- training(diabetes_bin_split)
diabetes_bin_test <- testing(diabetes_bin_split)

```
Creating 5-fold cross validation
```{r}
diabetes_bin_cv <-vfold_cv(diabetes_bin_train, 5)

```
Creating recipe
```{r}
tree_recipe <- recipe(Diabetes_binary ~ BMI + Age + Sex + PhysActivity + HighChol + GenHlth , 
                      data = diabetes_bin) |>
  step_normalize(BMI) |>
  step_dummy(Age, Sex, PhysActivity, HighChol, GenHlth)

tree_recipe |>
  prep(diabetes_bin_train) |>
  bake(diabetes_bin_train)
```
Using engine to set up model
```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
tree_mod
```
Creating the workflow
```{r}
tree_wkf <- workflow() |>
  add_recipe(tree_recipe) |>
  add_model(tree_mod)

```
Fit the workflow
```{r}
tree_fit <- tree_wkf |>
  tune_grid(resamples = diabetes_bin_cv, metrics = metric_set(mn_log_loss))
```
Collect metrics
```{r}
tree_fit |>
  collect_metrics() |>
  arrange(mean)
```
Select the best fit
```{r}
tree_best <- select_best(tree_fit, metric ="mn_log_loss")
tree_best

```
Finalize workflow and get the final fit 
```{r}
tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best)

tree_final_fit <- tree_final_wkf |>
  last_fit(diabetes_bin_split, metrics = metric_set(mn_log_loss))
tree_final_fit

```
```{r}
tree_final_fit |>
  collect_metrics()
```
## Random forest
Random forest creates multiple decision trees for both classification and regression tasks. A group of multiple predictors gives a better prediction than the best individual predictor so this is also called ensemble learning. Since random forest can average across many fitted trees, there is less variance than classification tree.

Create random forest model
```{r}
rand_mod <- rand_forest(mtry = tune(), trees = 100) |>
  set_engine("ranger") |>
  set_mode("classification")
rand_mod

```
Creating workflow
```{r}
ran_wfk <- workflow() |>
  add_recipe(tree_recipe) |>
  add_model(rand_mod)
ran_wfk
```
Fitting model to CV folds
```{r}
ran_fit <- ran_wfk |>
  tune_grid(resamples = diabetes_bin_cv, 
            grid = 4,
            metrics = metric_set(mn_log_loss))
```
Collect metric from fit
```{r}
ran_fit |>
  collect_metrics() |>
  arrange(mean)
```
Selecting the best model
```{r}
ran_best <- select_best(ran_fit, metric = "mn_log_loss" )
ran_best
```
Fitting the best model on the test set
```{r}
ran_final_wkf <- ran_wfk |>
  finalize_workflow(ran_best)
ran_final_fit <- ran_final_wkf |>
  last_fit(diabetes_bin_split, metrics = metric_set(mn_log_loss))
ran_final_fit
```
Collecting metric
```{r}
ran_final_fit |>
  collect_metrics()
```
After fitting both models on the test set, random forest is the best model using the least log loss metric.

Fitting the best random forest model to the entire data set
```{r}
final_model <- ran_final_wkf |>
  fit(diabetes_bin)
final_model
saveRDS(final_model, file="final_model.RDS")

```
```{r}
pred <- function(BMI = 28.38, Age = "60-64" , Sex = "Female", PhysActivity = "Yes", HighChol = "No high colesterol", GenHlth = "Very good") {
  bmi <- as.numeric(BMI)
  new_data <- as_tibble(data.frame(BMI = c(bmi), Age = c(Age), Sex = c(Sex), PhysActivity = c(PhysActivity), HighChol = HighChol, GenHlth = GenHlth))
  predict(final_model, new_data)
}
pred()

```









