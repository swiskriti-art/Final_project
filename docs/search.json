[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "This data have binary response variable diabetes_binary that gives whether there is diabetes or not. Our response variable is a categorical variable so a classification task, when we have categorical variable as response like success or failure we can use logistic regression model for predictions.\nWe will be using Classification Tree and Random Forest to create models for the Diabetes_binary variable. We will be using the following as predictors: BMI, Age, Sex, PhysActivity, HighChol, GenHlth variable. After that we will select the best out the two models, using log-loss as metric."
  },
  {
    "objectID": "Modeling.html#modeling",
    "href": "Modeling.html#modeling",
    "title": "Modeling",
    "section": "",
    "text": "This data have binary response variable diabetes_binary that gives whether there is diabetes or not. Our response variable is a categorical variable so a classification task, when we have categorical variable as response like success or failure we can use logistic regression model for predictions.\nWe will be using Classification Tree and Random Forest to create models for the Diabetes_binary variable. We will be using the following as predictors: BMI, Age, Sex, PhysActivity, HighChol, GenHlth variable. After that we will select the best out the two models, using log-loss as metric."
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\nClassification tree allows us to classify group membership. It tries to split up the predictor space into regions forming a tree and use the most prevalent class in the region as prediction.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.5.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n\n\nWarning: package 'dials' was built under R version 4.5.2\n\n\nWarning: package 'infer' was built under R version 4.5.2\n\n\nWarning: package 'modeldata' was built under R version 4.5.2\n\n\nWarning: package 'parsnip' was built under R version 4.5.2\n\n\nWarning: package 'recipes' was built under R version 4.5.2\n\n\nWarning: package 'rsample' was built under R version 4.5.2\n\n\nWarning: package 'tailor' was built under R version 4.5.2\n\n\nWarning: package 'tune' was built under R version 4.5.2\n\n\nWarning: package 'workflows' was built under R version 4.5.2\n\n\nWarning: package 'workflowsets' was built under R version 4.5.2\n\n\nWarning: package 'yardstick' was built under R version 4.5.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nset.seed(10)\n\nReading the diabetes data from EDA\n\ndiabetes_bin &lt;- readRDS(file = \"diabetes_bin.RDS\")\n\nSplitting data in training and testing set\n\ndiabetes_bin_split &lt;- initial_split(diabetes_bin, prop = 0.7)\ndiabetes_bin_train &lt;- training(diabetes_bin_split)\ndiabetes_bin_test &lt;- testing(diabetes_bin_split)\n\nCreating 5-fold cross validation\n\ndiabetes_bin_cv &lt;-vfold_cv(diabetes_bin_train, 5)\n\nCreating recipe\n\ntree_recipe &lt;- recipe(Diabetes_binary ~ BMI + Age + Sex + PhysActivity + HighChol + GenHlth , \n                      data = diabetes_bin) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(Age, Sex, PhysActivity, HighChol, GenHlth)\n\ntree_recipe |&gt;\n  prep(diabetes_bin_train) |&gt;\n  bake(diabetes_bin_train)\n\n# A tibble: 177,576 × 21\n      BMI Diabetes_binary Age_X25.29 Age_X30.34 Age_X35.39 Age_X40.44 Age_X45.49\n    &lt;dbl&gt; &lt;fct&gt;                &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 -0.663 No diabetes              0          0          0          1          0\n 2 -1.57  No diabetes              0          0          1          0          0\n 3 -0.813 No diabetes              0          0          0          0          0\n 4  0.241 No diabetes              0          0          0          0          0\n 5 -0.512 No diabetes              0          0          0          0          0\n 6 -0.361 No diabetes              0          0          0          0          0\n 7 -1.27  No diabetes              1          0          0          0          0\n 8 -0.964 No diabetes              0          0          0          0          0\n 9 -0.361 No diabetes              0          0          0          0          1\n10 -0.813 Diabetes                 0          0          0          0          0\n# ℹ 177,566 more rows\n# ℹ 14 more variables: Age_X50.54 &lt;dbl&gt;, Age_X55.59 &lt;dbl&gt;, Age_X60.64 &lt;dbl&gt;,\n#   Age_X65.69 &lt;dbl&gt;, Age_X70.74 &lt;dbl&gt;, Age_X75.79 &lt;dbl&gt;, Age_X..80 &lt;dbl&gt;,\n#   Sex_Male &lt;dbl&gt;, PhysActivity_Yes &lt;dbl&gt;, HighChol_High.colesterol &lt;dbl&gt;,\n#   GenHlth_Very.good &lt;dbl&gt;, GenHlth_Good &lt;dbl&gt;, GenHlth_Fair &lt;dbl&gt;,\n#   GenHlth_Poor &lt;dbl&gt;\n\n\nUsing engine to set up model\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\ntree_mod\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\nCreating the workflow\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_recipe) |&gt;\n  add_model(tree_mod)\n\nFit the workflow\n\ntree_fit &lt;- tree_wkf |&gt;\n  tune_grid(resamples = diabetes_bin_cv, metrics = metric_set(mn_log_loss))\n\nCollect metrics\n\ntree_fit |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.000001             13 mn_log_loss binary     0.344     5 1.94e-3 pre0_m…\n 2    0.000000001          11 mn_log_loss binary     0.346     5 1.90e-3 pre0_m…\n 3    0.0000001             7 mn_log_loss binary     0.362     5 3.51e-3 pre0_m…\n 4    0.0001                8 mn_log_loss binary     0.373     5 1.27e-3 pre0_m…\n 5    0.0000000001          5 mn_log_loss binary     0.374     5 1.24e-3 pre0_m…\n 6    0.001                15 mn_log_loss binary     0.375     5 9.37e-4 pre0_m…\n 7    0.00000001            1 mn_log_loss binary     0.404     5 7.20e-4 pre0_m…\n 8    0.00001               2 mn_log_loss binary     0.404     5 7.20e-4 pre0_m…\n 9    0.01                  4 mn_log_loss binary     0.404     5 7.20e-4 pre0_m…\n10    0.1                  10 mn_log_loss binary     0.404     5 7.20e-4 pre0_m…\n\n\nSelect the best fit\n\ntree_best &lt;- select_best(tree_fit, metric =\"mn_log_loss\")\ntree_best\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1        0.000001         13 pre0_mod05_post0\n\n\nFinalize workflow and get the final fit\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(diabetes_bin_split, metrics = metric_set(mn_log_loss))\ntree_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177576/76104]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.345 pre0_mod0_post0"
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling",
    "section": "Random forest",
    "text": "Random forest\nRandom forest creates multiple decision trees for both classification and regression tasks. A group of multiple predictors gives a better prediction than the best individual predictor so this is also called ensemble learning. Since random forest can average across many fitted trees, there is less variance than classification tree.\nCreate random forest model\n\nrand_mod &lt;- rand_forest(mtry = tune(), trees = 100) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\nrand_mod\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 100\n\nComputational engine: ranger \n\n\nCreating workflow\n\nran_wfk &lt;- workflow() |&gt;\n  add_recipe(tree_recipe) |&gt;\n  add_model(rand_mod)\nran_wfk\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 100\n\nComputational engine: ranger \n\n\nFitting model to CV folds\n\nran_fit &lt;- ran_wfk |&gt;\n  tune_grid(resamples = diabetes_bin_cv, \n            grid = 4,\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\n\nCollect metric from fit\n\nran_fit |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 4 × 7\n   mtry .metric     .estimator  mean     n  std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1     7 mn_log_loss binary     0.325     5 0.000861 pre0_mod2_post0\n2    14 mn_log_loss binary     0.344     5 0.00133  pre0_mod3_post0\n3     1 mn_log_loss binary     0.365     5 0.000609 pre0_mod1_post0\n4    20 mn_log_loss binary     0.506     5 0.00655  pre0_mod4_post0\n\n\nSelecting the best model\n\nran_best &lt;- select_best(ran_fit, metric = \"mn_log_loss\" )\nran_best\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;int&gt; &lt;chr&gt;          \n1     7 pre0_mod2_post0\n\n\nFitting the best model on the test set\n\nran_final_wkf &lt;- ran_wfk |&gt;\n  finalize_workflow(ran_best)\nran_final_fit &lt;- ran_final_wkf |&gt;\n  last_fit(diabetes_bin_split, metrics = metric_set(mn_log_loss))\nran_final_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                 id            .metrics .notes   .predictions .workflow \n  &lt;list&gt;                 &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [177576/76104]&gt; train/test s… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nCollecting metric\n\nran_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.326 pre0_mod0_post0\n\n\nAfter fitting both models on the test set, random forest is the best model using the least log loss metric.\nFitting the best random forest model to the entire data set\n\nfinal_model &lt;- ran_final_wkf |&gt;\n  fit(diabetes_bin)\nfinal_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~7L,      x), num.trees = ~100, num.threads = 1, verbose = FALSE, seed = sample.int(10^5,      1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  100 \nSample size:                      253680 \nNumber of independent variables:  20 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.09993917 \n\n\nSaving data for API\n\nsaveRDS(final_model, file=\"final_model.RDS\")\nsaveRDS(diabetes_bin, file=\"diabetes_bin.RDS\")\n\nConfusion matrix plot for the predictions against the actual data\n\npreds &lt;- predict(final_model, new_data = diabetes_bin, type = \"class\")\ncf &lt;- conf_mat(diabetes_bin |&gt; mutate(estimate = preds |&gt; pull()),\n         Diabetes_binary, \n         estimate)\nautoplot(cf, type = \"heatmap\")"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Reading the csv file\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary (readr)"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "Introduction",
    "text": "Introduction\nThe data that we are analyzing is a diabetes data from Diabetes Health Indicators Dataset , specifically the diabetes_binary_health_indicators_BRFSS2015.csv which contains binary diabetes indicators. It provides data on whether a person has diabetes or not with other health/lifestyle related variables. It has 22 columns with 3 numeric and other categorical variables.\nUsing the data we are analyzing what factors affect diabetes. We will look at some key variables and analyze their relationship with diabetes occurrence. Diabetes_binary variable gives us the diabetes or not. We will analyze the following variables against the Diabetes_binary variable: - BMI: This variable provides the body mass index - Age: This variable provides the age group - Income: This variable provides the income range - GenHlth: This variable provides the general health category - PhysActivity: This variable provides whether there was physical activity in the past 30 days - HighChol: This variable provides the cholesterol level\n\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiabetes_data\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\n\ndiabetes_bin &lt;- diabetes_data |&gt;\n  mutate(Diabetes_binary = factor (Diabetes_binary, levels = c (0,1),\n                  labels = c (\"No diabetes\", \"Diabetes\")),\n    HighBP = factor (HighBP, levels = c(0, 1), \n                    labels = c (\"No High BP\", \"High BP\")),\n    HighChol= factor(HighChol, levels = c(0,1), \n                     labels = c (\"No high colesterol\", \"High colesterol\")),\n    CholCheck = factor(CholCheck, levels = c(0, 1),\n                    labels = c( \"No check in 5 years\", \"Yes check in 5 years\" )),\n    Smoker = factor(Smoker, levels = c(0,1),\n                   labels = c(\"No\",\"Yes\")),\n    Stroke = factor(Stroke,levels = c(0,1),\n                    labels = c (\"No\",\"Yes\")),\n   \n    HeartDiseaseorAttack= factor(HeartDiseaseorAttack, levels=c(0,1),\n                     labels = c(\"No\",\"Yes\")),\n   \n    PhysActivity= factor(PhysActivity, levels=c(0,1),\n                          labels = c(\"No\",\"Yes\")),\n    Fruits = factor(Fruits, levels=c(0,1),\n             labels = c(\"0/day\",\"1 or more/day\")),\n    Veggies = factor(Veggies, levels=c(0,1),\n                      labels = c(\"0/day\",\"1 or more/day\")),\n    HvyAlcoholConsump= factor (HvyAlcoholConsump, levels=c(0,1),\n                     labels = c(\"No\",\"Yes\")),\n    AnyHealthcare = factor(AnyHealthcare, levels=c(0,1),\n                          labels = c(\"No\",\"Yes\")),\n    NoDocbcCost = factor(NoDocbcCost, levels=c(0,1), \n                          labels = c(\"No doc visit past yr\",\"Yes doc visit\")), \n    GenHlth = factor(GenHlth, levels=c(1,2,3,4,5), \n                      labels = c(\"Excellent\", \"Very good\",\"Good\",\"Fair\", \"Poor\")),  \n    DiffWalk = factor(DiffWalk, levels=c(0,1),\n                       labels = c(\"Not Difficult\",\"Difficult\")),\n    Sex = factor(Sex, levels=c(0,1),\n                 labels = c(\"Female\",\"Male\")),\n    Age = factor(Age, levels = 1:13, \n                labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\",\n                           \"45-49\",\"50-54\",\"55-59\",\"60-64\",\"65-69\",\"70-74\",\"75-79\",\"&gt;=80\")),\n    Education = factor(Education, levels = 1:6,\n                        labels = c(\"No school or only K\", \"Grades 1-8\", \"Grades 9-11\",\"Grade 12 or GED\", \"College 1-3 yrs\", \"College 4 yrs or more\")),\n    Income = factor(Income, levels = 1:8, \n                    labels = c(\" &lt;$10000\", \" &gt;=10000 & &lt;15000\", \"&gt;=15000 & &lt;20000\", \"&gt;=20000 & &lt;25000\", \"&gt;=25000 & &lt;35000\", \"&gt;=35000 & &lt;50000\", \"&gt;=50000 & &lt;75000\", \"&gt;=75000\"))\n  )\ndiabetes_bin\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP     HighChol           CholCheck     BMI Smoker Stroke\n   &lt;fct&gt;           &lt;fct&gt;      &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No diabetes     High BP    High colesterol    Yes check …    40 Yes    No    \n 2 No diabetes     No High BP No high colesterol No check i…    25 Yes    No    \n 3 No diabetes     High BP    High colesterol    Yes check …    28 No     No    \n 4 No diabetes     High BP    No high colesterol Yes check …    27 No     No    \n 5 No diabetes     High BP    High colesterol    Yes check …    24 No     No    \n 6 No diabetes     High BP    High colesterol    Yes check …    25 Yes    No    \n 7 No diabetes     High BP    No high colesterol Yes check …    30 Yes    No    \n 8 No diabetes     High BP    High colesterol    Yes check …    25 Yes    No    \n 9 Diabetes        High BP    High colesterol    Yes check …    30 Yes    No    \n10 No diabetes     No High BP No high colesterol Yes check …    24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;fct&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;fct&gt;,\n#   NoDocbcCost &lt;fct&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;fct&gt;, Sex &lt;fct&gt;, Age &lt;fct&gt;, Education &lt;fct&gt;, Income &lt;fct&gt;"
  },
  {
    "objectID": "EDA.html#checking-the-data",
    "href": "EDA.html#checking-the-data",
    "title": "EDA",
    "section": "Checking the data",
    "text": "Checking the data\n\ndiabetes_bin |&gt;\n is.na () |&gt;\n colSums()\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nSummarize numeric column to see value distribution\n\nsummary(diabetes_bin |&gt;\n          select(where(is.numeric))\n        )\n\n      BMI           MentHlth         PhysHlth     \n Min.   :12.00   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.:24.00   1st Qu.: 0.000   1st Qu.: 0.000  \n Median :27.00   Median : 0.000   Median : 0.000  \n Mean   :28.38   Mean   : 3.185   Mean   : 4.242  \n 3rd Qu.:31.00   3rd Qu.: 2.000   3rd Qu.: 3.000  \n Max.   :98.00   Max.   :30.000   Max.   :30.000  \n\n\nLooking at numeric data summaries, the BMI seems about right.The mental health and physical health (illness) days shows less number of overall illness.\nChecking for uniqueness of data\n\ntable(diabetes_bin$Income)\n\n\n          &lt;$10000  &gt;=10000 & &lt;15000  &gt;=15000 & &lt;20000  &gt;=20000 & &lt;25000 \n             9811             11783             15994             20135 \n &gt;=25000 & &lt;35000  &gt;=35000 & &lt;50000  &gt;=50000 & &lt;75000           &gt;=75000 \n            25883             36470             43219             90385 \n\n\nChecking data relationship with some variables\n\ntable(diabetes_bin$Diabetes_binary, diabetes_bin$Age)\n\n             \n              18-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74\n  No diabetes  5622  7458 10809 13197 15106 18077 23226 26569 27511 25636 18392\n  Diabetes       78   140   314   626  1051  1742  3088  4263  5733  6558  5141\n             \n              75-79  &gt;=80\n  No diabetes 12577 14154\n  Diabetes     3403  3209\n\ntable(diabetes_bin$Age)\n\n\n18-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74 75-79  &gt;=80 \n 5700  7598 11123 13823 16157 19819 26314 30832 33244 32194 23533 15980 17363 \n\n\nWe can see that that as the age increases there are more chances to have diabetes, data seems to be normal.\n\ntable(diabetes_bin$Diabetes_binary, diabetes_bin$PhysActivity)\n\n             \n                  No    Yes\n  No diabetes  48701 169633\n  Diabetes     13059  22287\n\n\nLooking at past 30 days physical activity, we can’t say anything about diabetes occurrence.\n\ntable(diabetes_bin$Diabetes_binary, diabetes_bin$Sex)\n\n             \n              Female   Male\n  No diabetes 123563  94771\n  Diabetes     18411  16935\n\n\nLooking at the sex breakdown, there seems to be a little more prevalence of diabetes in male than female.\n\ntable(diabetes_bin$Diabetes_binary, diabetes_bin$HighChol)\n\n             \n              No high colesterol High colesterol\n  No diabetes             134429           83905\n  Diabetes                 11660           23686\n\n\nFrom the above, there seems to be some correlation between high cholesterol and diabetes."
  },
  {
    "objectID": "EDA.html#data-visualization",
    "href": "EDA.html#data-visualization",
    "title": "EDA",
    "section": "Data visualization",
    "text": "Data visualization\nCreating the box plots for categorical variables\n\nggplot(data=diabetes_bin,\n       aes(x=BMI, y=Diabetes_binary)) +\n  geom_boxplot() +\n  labs(y = \"\")\n\n\n\n\n\n\n\n\nFrom the above BMI box plot above, diabetes occurrence is sightly higher on average for higher BMI than those with no diabetes\n\nggplot(data=diabetes_bin,\n       aes(x=Diabetes_binary, fill = Income )) +\n  geom_bar() +\n  facet_wrap(diabetes_bin$Income) +\n  labs(x = \"\")\n\n\n\n\n\n\n\n\nFrom the above plot, income doesn’t seem to have much correlation with having diabetes or not.\n\nggplot(data=diabetes_bin,\n       aes(x=Diabetes_binary, fill = GenHlth )) +\n  geom_bar() +\n  facet_wrap(diabetes_bin$GenHlth) +\n  labs(x = \"\")\n\n\n\n\n\n\n\n\nComparing general health and diabetes, we can say when the General health is good, there is less chances of having diabetes, however we cannot say anything about when the General health is poor and fair. Because no diabetes and diabetes both looks equal in fair and poor general health.\nSaving data\n\nsaveRDS(diabetes_bin, \"diabetes_bin.RDS\")\n\nClick here for the Modeling Page"
  }
]